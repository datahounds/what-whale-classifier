{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requirements\n",
    "Training data should be arranged in the following structure:\n",
    "\n",
    "```\n",
    "data\n",
    "-- train\n",
    "---- class1\n",
    "------ img1.jpg\n",
    "------ img2.jpg\n",
    "---- class2\n",
    "---- ...\n",
    "-- val\n",
    "---- class1\n",
    "------ img101.jpg\n",
    "------ img102.jpg\n",
    "---- class2\n",
    "---- ...\n",
    "-- test\n",
    "---- img201.jpg\n",
    "---- img202.jpg\n",
    "---- ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if host machine has GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images\n",
    "#### Apply transforms\n",
    "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are [0.485, 0.456, 0.406] and the standard deviations are [0.229, 0.224, 0.225]. (see these same values used in an official PyTorch example [here](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Just normalization for validation\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train = datasets.ImageFolder(data_dir+'/train', transform=train_transforms)\n",
    "test = datasets.ImageFolder(data_dir+'/val', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=8, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {}\n",
    "n_classes = 0\n",
    "for c in train.classes:\n",
    "    class_mapping[n_classes] = c\n",
    "    n_classes += 1\n",
    "\n",
    "print(n_classes)\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = torchvision.utils.make_grid(inp)\n",
    "    # in tensor, image is (batch, width, height)\n",
    "    # so you have to transpose it to (width, height, batch) \n",
    "    # in numpy to show it.\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # de-normalise images to get original colours\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(9,9))\n",
    "    plt.imshow(inp)\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, labels = next(iter(trainloader))\n",
    "# show images in batch\n",
    "imshow(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained mobilenet model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the head of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the new head fits onto the existing body\n",
    "# index added as model.classifier returns list: second index contains in_features\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "# define architecture of classifier head\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, n_classes),\n",
    "    nn.LogSoftmax(dim=-1)\n",
    ")\n",
    "\n",
    "# update the head on pretrained model\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # turns on dropout\n",
    "    model.train()\n",
    "    \n",
    "    batch = 1\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)  # forward pass\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backprop\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        losses.append(running_loss / batch)\n",
    "        batch += 1\n",
    "    print(f'Epoch {epoch+1} training loss: {running_loss / batch:.3f}')\n",
    "    \n",
    "    # VALIDATION AT END OF EPOCH\n",
    "    # turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # turn off dropout\n",
    "        model.eval()\n",
    "        test_batch = 1\n",
    "        val_loss = 0.0\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).to('cpu')\n",
    "            loss = criterion(outputs, labels.to('cpu'))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, batch_predictions = torch.max(outputs.data, 1)\n",
    "            y_pred = np.hstack((y_pred, batch_predictions.numpy()))\n",
    "            y_true = np.hstack((y_true, labels.to('cpu').numpy()))\n",
    "            test_batch += 1\n",
    "\n",
    "        print(f'Validation accuracy: {100*(y_true == y_pred).mean():.3f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='loss')\n",
    "plt.legend()\n",
    "plt.title('training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for re-use on other devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make model name dynamic\n",
    "torch.save(model,'./models/mobilenet.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some test images and show predicted labels\n",
    "  - TODO: add true labels to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "samples, _ = iter(testloader).next()\n",
    "samples = samples.to(device)\n",
    "output = model(samples[:24])\n",
    "pred = torch.argmax(output, dim=1)\n",
    "pred = [p.item() for p in pred]\n",
    "\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "fig.tight_layout()\n",
    "for num, sample in enumerate(samples[:24]):\n",
    "    plt.subplot(4,6,num+1)\n",
    "    plt.title(class_mapping[pred[num]])\n",
    "    plt.axis('off')\n",
    "    sample = sample.cpu().numpy()\n",
    "    sample = np.transpose(sample, (1,2,0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    sample = std * sample + mean\n",
    "    sample = np.clip(sample, 0, 1)\n",
    "    plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed accuracy assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(n_classes, n_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(testloader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit9a2292bfc8a343529cca1d97b8813480"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}